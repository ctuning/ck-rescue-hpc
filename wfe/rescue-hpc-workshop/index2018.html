<!------------------------------------------------>
<div style="float:right;width:400px;padding:10px;">
 <div id="ck_box_with_shadow1">

  <span style="font-size:medium;">

   <center>
    <b>Important Dates</b><br>
   </center>

   <table border="0" cellpadding="2" cellspacing="0">
    <tr><td valign="top">Paper submission: </td><td valign="top"><b><strike>5 September 2018</strike></b></td></tr>
    <tr><td valign="top">Acceptance notification: </td><td valign="top"><b><strike>21 September 2018</strike></b></td></tr>
    <tr><td valign="top">Workshop: </td><td valign="top"><b>11 November 2018<br>(<a href="$#ck_root_page_url#$program2018$#ck_page_suffix#$">program</a>)</b></td></tr>
   </table>

  </span>

 </div>

  <!-- <gcse:searchbox-only></gcse:searchbox-only> -->

</div>

<p>
<ul>
</ul>

<!------------------------------------------------>
<h2>News</h2>

Workshop program is now available <b><a href="$#ck_root_page_url#$program2018$#ck_page_suffix#$">online</a></b> - looking forward to see you in Dallas!

<!------------------------------------------------>
<h2>Introduction</h2>

<p>
Reproducibility is critical for the scientific process. Sharing the
artifacts, data, and workflows associated with papers forces authors to
turn a more careful eye to their own work, and it enables other
scientists to easily validate and build on prior work. Over the past five
years, many top-tier parallel computing conferences have
established <a href="http://cTuning.org/ae">Artifact Evaluation (AE)</a>
initiatives.  The community has bought in, and nearly half of accepted
papers now include artifacts.

<p>
Unfortunately, recent attempts to
reproduce experimental results show that many challenges still remain 
(<a href="https://www.slideshare.net/GrigoriFursin/enabling-open-and-reproducible-computer-systems-research-the-good-the-bad-and-the-ugly">AE slides</a>,
<a href="https://tinyurl.com/y8v5tdsy">ACM ReQuEST report</a>). 

A lack of common tools, along with
increasingly deep stacks of dependencies, lead to ad-hoc workflows, and
evaluators struggle to install, run, and analyze experiments.  These
challenges are not unique to artifact evaluation. Users of production
simulation codes struggle to reproduce complex workflows, even on the
same machine. Benchmark suites are notoriously complex to configure and
work with, and reproducing their performance can be a daunting task.
Indeed, nearly all shared artifacts still require manual steps and human
intuition, which ultimately makes them difficult to customize, port,
reuse, and build upon.

<p>
ResCuE-HPC will bring together HPC researchers and practitioners to
propose and discuss ways to enable reproducible, portable and
customizable experimental workflows for HPC.  We are interested in
contributions that describe state-of-the-art and pitfalls for
reproducibility, as well as improvements to existing frameworks,
benchmarks and datasets that can be used to run HPC workloads across
multiple software versions and hardware architectures.  Ultimately, we
aim to automate artifact evaluation, benchmarking, and workflows with a
common co-design framework, and collaboratively solve reproducibility
issues in HPC.

<h2>Topics of Interest</h2>
<p>
We invite position papers of up to 4 pages presenting novel or existing
practical solutions to:

<ul>
<li> automate and unify artifact evaluation at HPC conferences;

<li> share artifacts (workloads, benchmarks, data sets, models, tools),
  workflows and experiments in a portable, customizable, and reusable
  format;

<li> automatically and natively install and rebuild all software
  dependencies required for shared experimental workflows on different
  machines and environments;

<li> automatically report and visualize experimental results including
  interactive articles to assist reproducible initiative at SC and other
  conferences and journals;

<li> continuously validate experiments from past research and
  report/record unexpected behavior (bugs, numerical instability,
  variation of empirical results such as execution time or energy
  measurements, etc) on new and evolving software and hardware stack;

<li> establish open repositories of common benchmarks, data sets and
  tools to accelerate knowledge exchange between HPC centers;

<li> enable universal, customizable and multi-objective auto-tuning
  and co-design of HPC software and hardware in a reproducible
  and reusable way;
 
<li> unify statistical analysis and predictive
  modeling techniques to improve reproducibility of empirical
  experimental results.
</ul>

We also encourage submissions demonstrating practical use-cases
of portable, customizable and reusable HPC workflows by connecting
together existing tools including but not limited to
<a href="https://spack.io">Spack</a>
(see shared <a href="http://spack.readthedocs.io/en/latest/package_list.html">packages</a>),
<a href="https://github.com/ctuning/ck">Collective Knowledge (CK)</a>
(see shared <a href="http://cKnowledge.org/shared-repos.html">repositories with CK workflows</a>, 
<a href="http://cKnowledge.org/shared-programs.html">unified programs</a>, 
<a href="http://cKnowledge.org/shared-soft-detection-plugins.html">soft detection plugins<a/>, 
<a href="http://cKnowledge.org/shared-packages.html">packages</a>),
<a href="https://github.com/easybuilders/easybuild">EasyBuild</a>
(see shared <a href="http://easybuild.readthedocs.io/en/latest/version-specific/Supported_software.html">packages</a>),
<a href="https://www.commonwl.org">Common Workflow Language</a>
and many others.

<h2>Format</h2>

<p>
The day will be organized into sessions of 3-4 related papers. To spark
discussion, Each author will briefly introduce their techniques (10-15
minutes), and this will be followed by an open panel including the
audience.

<p>
There will be no formal proceedings for the first edition of this
workshop!  Instead, all ResCuE-HPC authors will be able to participate in
preparation of a single ResCuE-HPC report. The report will focus on
gradual convergence on a common experimental methodology, as well as
possible formats for workflows and artifact sharing (meta information and
API).

We plan to make this report available to reproducibility and artifact
evaluation chairs at the leading HPC, ML and systems conferences as well
as
the <a href="https://www.acm.org/publications/task-force-on-data-software-and-reproducibility">ACM
task force on reproducibility</a> where we are founding members.

<p>
We hope that ResCuE-HPC workshop will help the community to gradually
converge on a common experimental methodology and possible formats for
workflows and artifact sharing (meta information and API).  This, in
turn, should help our community unify and automate Artifact Evaluation,
benchmarking, and workflows. The resulting ability to quickly prototype
research ideas will dramatically accelerate development of the next
generation of HPC software and hardware by reusing prior work.

<!------------------------------------------------>
<h2>Important dates</h2>

<ul>
 <li>Paper submission: <b>24 August 2018</b>
 <li>Acceptance notification: <b>21 September 2018</b>
 <li>Workshop: <b>11 November 2018</b>
</ul>

<p>
Check <a href="http://sc18.supercomputing.org">SC18 home page</a>
for registration deadlines and other related information.

<!------------------------------------------------>
<h2>Submissions</h2>

<p>
Authors must submit their position papers (max 4 pages)
using double-column, single-spaced letter format
<i><pre>\documentclass[10pt,letterpaper,twocolumn]{article}</pre></i> as
PDF files.

<p>
All papers should be submitted via
<a href="https://submissions.supercomputing.org/?page=Submit&id=SC18WorkshopResCuEHPC&site=sc18">SC18 submission website</a>.

Submissions are single-blind and will be peer-reviewed.

<!------------------------------------------------>
<h2>Questions</h2>

<p>
If you have any questions or comments, don't hesitate to
contact <a href="mailto:grigori.fursin@cTuning.org;gamblin2@llnl.gov">workshop
organizers</a>!
